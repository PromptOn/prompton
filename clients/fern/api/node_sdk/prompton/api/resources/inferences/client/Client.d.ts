/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as core from "../../../../core";
import * as PromptonApi from "../../..";
export declare namespace Inferences {
    interface Options {
        environment: core.Supplier<string>;
        token?: core.Supplier<core.BearerToken | undefined>;
    }
}
export declare class Inferences {
    protected readonly _options: Inferences.Options;
    constructor(_options: Inferences.Options);
    /**
     * @throws {@link PromptonApi.BadRequestError}
     * @throws {@link PromptonApi.UnauthorizedError}
     * @throws {@link PromptonApi.NotFoundError}
     * @throws {@link PromptonApi.UnprocessableEntityError}
     */
    getInferencesList(request?: PromptonApi.GetInferencesListRequest): Promise<PromptonApi.InferenceRead[]>;
    /**
     * The core of the functionality:
     * 1. Populating the template from prompt version with the passed values
     * 2. Logging the request
     * 3. Sending request to provider
     * 4. Logging response
     * 4. Returning response with `inference_id`
     *
     * You can specify which prompt version you want to use in two ways by setting on of:
     *
     *  - `prompt_version_id` - uses the specified prompt version
     *  - `prompt_id` - uses the `Live` status prompt version assigned to the given `prompt_id`. This allows to release new prompt versions using Prompton API if you only reference `prompt_id` in your client code.
     *
     *      If there are multiple  prompt versions in `Live` status for the prompt_id then it picks one randomly. It's useful for split testing.
     *      This method will return an error if there is no `Live` status message.
     *
     * It also handles errors, timeouts and sets the inference status accordingly. It will still process response if client disconnects before it arrives.
     *
     * _Note: raw request data is also logged, GET `inferences/{id}` reponse includes it as well._
     *
     * You can use a few easter eggs to test it without a valid api key:
     *
     *   - `"end_user_id": "mock_me_softly"`
     *   - `"end_user_id": "timeout_me_softly"`
     *   - `"end_user_id": "fail_me_softly"`
     * @throws {@link PromptonApi.BadRequestError}
     * @throws {@link PromptonApi.UnauthorizedError}
     * @throws {@link PromptonApi.UnprocessableEntityError}
     */
    newInference(request: PromptonApi.NewInferenceRequest): Promise<PromptonApi.InferencePostResponse>;
    /**
     * @throws {@link PromptonApi.BadRequestError}
     * @throws {@link PromptonApi.UnauthorizedError}
     * @throws {@link PromptonApi.NotFoundError}
     * @throws {@link PromptonApi.UnprocessableEntityError}
     */
    getInferenceById(id: string): Promise<PromptonApi.InferenceRead>;
    protected _getAuthorizationHeader(): Promise<string | undefined>;
}
