/**
 * prompton-api
 * API for prompton - managing full lifecycle of AI chat prompts.
 *
 * The version of the OpenAPI document: 0.0.1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

import { RequestFile } from './models';
import { InferenceRequestData } from './inferenceRequestData';
import { InferenceResponseStatus } from './inferenceResponseStatus';
import { Response } from './response';

export class InferenceRead {
    'id'?: string;
    'createdAt'?: Date;
    'createdByUserId'?: string;
    'createdByOrgId'?: string;
    /**
    * The API consumer\'s internal user reference for metrics. It is also relayed to the provider as part of the request if the provider supports it (eg. OpenAI\'s user field).
    */
    'endUserId'?: string;
    /**
    * The API consumer\'s source for metrics (e.g. AndroidApp etc).
    */
    'source'?: string;
    /**
    * The API consumer\'s internal reference id to able to link references to their sessions.
    */
    'clientRefId'?: string;
    'templateArgs'?: { [key: string]: string; };
    'metadata'?: object;
    /**
    * Provider request timout in seconds. If not provided, then Prompton API\'s default timeout for the provider will be used (90sec or `DEFAULT_OPENAI_REQUEST_TIMEOUT_SECONDS` env var if provided).
    */
    'requestTimeout'?: number;
    'promptVersionId': string;
    /**
    * If inference was by prompt_id then a list of all other prompt versions considered for this inference. I.e. all prompt versions in Live status at the time of the inference
    */
    'promptVersionIdsConsidered'?: Array<string>;
    'promptId': string;
    'promptVersionName'?: string;
    'status'?: InferenceResponseStatus;
    'request'?: InferenceRequestData;
    'response'?: Response;

    static discriminator: string | undefined = undefined;

    static attributeTypeMap: Array<{name: string, baseName: string, type: string}> = [
        {
            "name": "id",
            "baseName": "_id",
            "type": "string"
        },
        {
            "name": "createdAt",
            "baseName": "created_at",
            "type": "Date"
        },
        {
            "name": "createdByUserId",
            "baseName": "created_by_user_id",
            "type": "string"
        },
        {
            "name": "createdByOrgId",
            "baseName": "created_by_org_id",
            "type": "string"
        },
        {
            "name": "endUserId",
            "baseName": "end_user_id",
            "type": "string"
        },
        {
            "name": "source",
            "baseName": "source",
            "type": "string"
        },
        {
            "name": "clientRefId",
            "baseName": "client_ref_id",
            "type": "string"
        },
        {
            "name": "templateArgs",
            "baseName": "template_args",
            "type": "{ [key: string]: string; }"
        },
        {
            "name": "metadata",
            "baseName": "metadata",
            "type": "object"
        },
        {
            "name": "requestTimeout",
            "baseName": "request_timeout",
            "type": "number"
        },
        {
            "name": "promptVersionId",
            "baseName": "prompt_version_id",
            "type": "string"
        },
        {
            "name": "promptVersionIdsConsidered",
            "baseName": "prompt_version_ids_considered",
            "type": "Array<string>"
        },
        {
            "name": "promptId",
            "baseName": "prompt_id",
            "type": "string"
        },
        {
            "name": "promptVersionName",
            "baseName": "prompt_version_name",
            "type": "string"
        },
        {
            "name": "status",
            "baseName": "status",
            "type": "InferenceResponseStatus"
        },
        {
            "name": "request",
            "baseName": "request",
            "type": "InferenceRequestData"
        },
        {
            "name": "response",
            "baseName": "response",
            "type": "Response"
        }    ];

    static getAttributeTypeMap() {
        return InferenceRead.attributeTypeMap;
    }
}

export namespace InferenceRead {
}
